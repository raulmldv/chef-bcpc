[global]
fsid = <%= @config['ceph']['fsid'] %>
public network = <%= @public_network.join(',') %>
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
mon initial members = <%= @storageheadnodes.map{ |n| n['hostname'] }.join(',') %>
mon host = <%= @storageheadnodes.map{ |n| "[v2:#{n['service_ip']}:3300]" }.join(',') %>
ms_bind_msgr1 = false
max open files = <%= node['bcpc']['ceph']['max_open_files'] %>
rbd default features = <%= node['bcpc']['ceph']['rbd_default_features'] %>
osd crush initial weight = <%= node['bcpc']['ceph']['osd_crush_initial_weight'] %>
admin socket mode = 0775
mon osd down out interval = <%= node['bcpc']['ceph']['mon_osd_down_out_interval'] %>
mon max pg per osd = <%= node['bcpc']['ceph']['mon_max_pg_per_osd'] %>

<% if @is_bootstrapping %>
# Chef controls the size, generally. However, during cluster bootstrapping,
# we can only afford a very limited number of total PGs as we don't bootstrap
# OSDs until after the pools are created presently, and Ceph will block us
# from creating pools when there are "too many" total PGs and no OSDs joined
# to the cluster yet. To limit # of PGs, lock size to 1 while cluster builds.
mon allow pool size one = true
osd pool default size = 1

<% end %>
# settings to throttle OSD scrubbing visible to both mgrs and OSDs
osd deep scrub interval = <%= @node['bcpc']['ceph']['osd_deep_scrub_interval'] %>
osd scrub max interval = <%= @node['bcpc']['ceph']['osd_scrub_max_interval'] %>

# Autoscaler control. This allows operators to optionally receive health
# warnings (or disregard suggestions altogether) when the PGs are deemed under
# or over-sized, rather than having Ceph act on them automatically.
osd pool default pg autoscale mode = <%= node['bcpc']['ceph']['osd_pool_default_pg_autoscale_mode'] %>

[mon]
auth allow insecure global id reclaim = <%= node['bcpc']['ceph']['mon_auth_allow_insecure_global_id_reclaim'] %>
mon compact on start = true
mon cpu threads = <%= node['bcpc']['ceph']['mon_cpu_threads'] %>
mon max pool pg num = <%= node['bcpc']['ceph']['mon_max_pool_pg_num'] %>
mon mgr beacon grace = <%= node['bcpc']['ceph']['mon_mgr_beacon_grace'] %>
mgr tick period = <%= node['bcpc']['ceph']['mgr_tick_period'] %>

[mgr]
mgr stats period = <%= node['bcpc']['ceph']['mgr_stats_period'] %>
mgr stats threshold = <%= node['bcpc']['ceph']['mgr_stats_threshold'] %>
mgr tick period = <%= node['bcpc']['ceph']['mgr_tick_period'] %>
ms tcp listen backlog = <%= node['bcpc']['ceph']['mgr_ms_tcp_listen_backlog'] %>

[osd]
osd scrub load threshold = <%= node['bcpc']['ceph']['osd_scrub_load_threshold'] %>
osd memory target = <%= node['bcpc']['ceph']['osd_memory_target'] %>

# settings to throttle OSD scrubbing
osd scrub begin hour = <%= @node['bcpc']['ceph']['osd_scrub_begin_hour'] %>
osd scrub end hour = <%= @node['bcpc']['ceph']['osd_scrub_end_hour'] %>
osd scrub sleep = <%= @node['bcpc']['ceph']['osd_scrub_sleep'] %>
osd scrub chunk min = <%= @node['bcpc']['ceph']['osd_scrub_chunk_min'] %>
osd scrub chunk max = <%= @node['bcpc']['ceph']['osd_scrub_chunk_max'] %>
osd scrub max scrubs = <%= @node['bcpc']['ceph']['osd_max_scrubs'] %>

# settings to throttle OSD recovery parameters
osd mon report interval min = <%= @node['bcpc']['ceph']['osd_mon_report_interval_min'] %>
osd recovery max active = <%= @node['bcpc']['ceph']['osd_recovery_max_active'] %>
osd recovery threads = <%= @node['bcpc']['ceph']['osd_recovery_threads'] %>
osd recovery op priority = <%= @node['bcpc']['ceph']['osd_recovery_op_priority'] %>
osd max backfills  = <%= @node['bcpc']['ceph']['osd_max_backfills'] %>
osd op threads = <%= @node['bcpc']['ceph']['osd_op_threads'] %>

# bluestore tuning
bluestore rocksdb options = <%= @node['bcpc']['ceph']['bluestore_rocksdb_options'].join(',') %>
bluestore cache size ssd = <%= @node['bcpc']['ceph']['bluestore_cache_size_ssd'] %>
bluestore fsck quick fix threads = <%= node['bcpc']['ceph']['bluestore_fsck_quick_fix_threads'] %>

# As of Octopus 15.2.12, the AVL block allocator can degrade performance
# when OSDs have fragmentated blocks, especially so when high IOP flash media
# backs bluestore.  Use bitmap allocators at the expense of memory consumption
# to avoid degrading I/O performance in such cases.
bluefs_allocator = bitmap
bluestore_allocator = bitmap

# Although 'bluefs buffered io' is default on (again) in Octopus 15.2.13,
# it has a history of being flipped on/off.  It provides a large performance
# advantage so long as the host has enough memory/does not swap; make sure
# it stays on regardless of what upstream does.
bluefs buffered io = true

# In preparation for Quincy, we are remaining on the WPQ scheduler until we
# have landed on the new release and tested mClock sufficiently.
osd op queue = wpq

# "fast" OSD shutdown, which is just an abrupt stop of the OSD instead of a
# "clean" stop, is a default feature of new Cephs. The mechanism bizzarely
# provides no notice ot the mon that the OSD is being stopped, and thus
# relies on timeouts, etc. to tear things down by default.
#
# This, implemented as is, results in seconds of I/O latency when OSDs stop:
# particularly on large clusters, which have the clog flooded with notices
# pertaining to unexpected death of an OSD.
#
# In order to resolve the latency issues associated with this, we instead
# notify the mon that we're shutting down (with haste) so that it's not quite
# so brutal. Though, we still abruptly stop.
#
# This is the default in Quincy, so we remove this once we land on it.
osd fast shutdown notify mon = true

<% @rbd_users.each do |user| %>
<%= "[client.#{user}]" %>
rbd cache = true
rbd cache writethrough until flush = true
rbd concurrent management ops = 20
<% end %>
