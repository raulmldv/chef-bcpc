[global]
fsid = <%= @config['ceph']['fsid'] %>
public network = <%= @public_network.join(',') %>
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
mon initial members = <%= @storageheadnodes.map{ |n| n['hostname'] }.join(',') %>
mon host = <%= @storageheadnodes.map{ |n| "[v2:#{n['service_ip']}:3300]" }.join(',') %>
ms_bind_msgr1 = false
rbd default features = <%= node['bcpc']['ceph']['rbd_default_features'] %>
osd crush initial weight = <%= node['bcpc']['ceph']['osd_crush_initial_weight'] %>
admin socket mode = 0775
mon osd down out interval = <%= node['bcpc']['ceph']['mon_osd_down_out_interval'] %>
mon max pg per osd = <%= node['bcpc']['ceph']['mon_max_pg_per_osd'] %>

<% if @is_bootstrapping %>
# Chef controls the size, generally. However, during cluster bootstrapping,
# we can only afford a very limited number of total PGs as we don't bootstrap
# OSDs until after the pools are created presently, and Ceph will block us
# from creating pools when there are "too many" total PGs and no OSDs joined
# to the cluster yet. To limit # of PGs, lock size to 1 while cluster builds.
mon allow pool size one = true
osd pool default size = 1

<% end %>
# settings to throttle OSD scrubbing visible to both mgrs and OSDs
osd deep scrub interval = <%= @node['bcpc']['ceph']['osd_deep_scrub_interval'] %>
osd scrub max interval = <%= @node['bcpc']['ceph']['osd_scrub_max_interval'] %>

# Autoscaler control. This allows operators to optionally receive health
# warnings (or disregard suggestions altogether) when the PGs are deemed under
# or over-sized, rather than having Ceph act on them automatically.
osd pool default pg autoscale mode = <%= node['bcpc']['ceph']['osd_pool_default_pg_autoscale_mode'] %>

[mon]
auth allow insecure global id reclaim = <%= node['bcpc']['ceph']['mon_auth_allow_insecure_global_id_reclaim'] %>
mon compact on start = true
mon cpu threads = <%= node['bcpc']['ceph']['mon_cpu_threads'] %>
mon max pool pg num = <%= node['bcpc']['ceph']['mon_max_pool_pg_num'] %>
mon mgr beacon grace = <%= node['bcpc']['ceph']['mon_mgr_beacon_grace'] %>
mgr tick period = <%= node['bcpc']['ceph']['mgr_tick_period'] %>

[mgr]
mgr stats period = <%= node['bcpc']['ceph']['mgr_stats_period'] %>
mgr stats threshold = <%= node['bcpc']['ceph']['mgr_stats_threshold'] %>
mgr tick period = <%= node['bcpc']['ceph']['mgr_tick_period'] %>
ms tcp listen backlog = <%= node['bcpc']['ceph']['mgr_ms_tcp_listen_backlog'] %>

[osd]
osd scrub load threshold = <%= node['bcpc']['ceph']['osd_scrub_load_threshold'] %>
osd memory target = <%= node['bcpc']['ceph']['osd_memory_target'] %>

# settings to throttle OSD scrubbing
osd scrub begin hour = <%= @node['bcpc']['ceph']['osd_scrub_begin_hour'] %>
osd scrub end hour = <%= @node['bcpc']['ceph']['osd_scrub_end_hour'] %>
osd scrub sleep = <%= @node['bcpc']['ceph']['osd_scrub_sleep'] %>
osd scrub chunk min = <%= @node['bcpc']['ceph']['osd_scrub_chunk_min'] %>
osd scrub chunk max = <%= @node['bcpc']['ceph']['osd_scrub_chunk_max'] %>
osd max scrubs = <%= @node['bcpc']['ceph']['osd_max_scrubs'] %>

# settings to throttle OSD recovery parameters
osd mon report interval = <%= @node['bcpc']['ceph']['osd_mon_report_interval'] %>
osd recovery max active = <%= @node['bcpc']['ceph']['osd_recovery_max_active'] %>
osd recovery op priority = <%= @node['bcpc']['ceph']['osd_recovery_op_priority'] %>
osd max backfills  = <%= @node['bcpc']['ceph']['osd_max_backfills'] %>

# bluestore tuning
bluestore rocksdb options = <%= @node['bcpc']['ceph']['bluestore_rocksdb_options'].join(',') %>

# As of Octopus 15.2.12, the AVL block allocator can degrade performance
# when OSDs have fragmentated blocks, especially so when high IOP flash media
# backs bluestore.  Use bitmap allocators at the expense of memory consumption
# to avoid degrading I/O performance in such cases.
bluefs_allocator = bitmap
bluestore_allocator = bitmap

# Although 'bluefs buffered io' is default on (again) in Octopus 15.2.13,
# it has a history of being flipped on/off.  It provides a large performance
# advantage so long as the host has enough memory/does not swap; make sure
# it stays on regardless of what upstream does.
bluefs buffered io = true

# In preparation for Quincy, we are remaining on the WPQ scheduler until we
# have landed on the new release and tested mClock sufficiently.
osd op queue = wpq

<% @rbd_users.each do |user| %>
<%= "[client.#{user}]" %>
rbd cache = true
rbd cache writethrough until flush = true
rbd concurrent management ops = 20
<% end %>
